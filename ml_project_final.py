# -*- coding: utf-8 -*-
"""ML-project-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lnv-87O9w7W41fiJllJ3kYRa13g3adkZ
"""

# Install required packages
!pip install convokit contractions

!pip install datasets

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from convokit import Corpus, download
import contractions

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from tqdm.auto import tqdm
import re
import random
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

"""# Emotion Detection"""

# For the first time
corpus = Corpus(filename=download(name="movie-corpus", data_dir='/content/drive/My Drive/ML-Colab-1'))

# Load corpus
corpus = Corpus(filename="/content/drive/My Drive/ML-Colab-1/movie-corpus")

# Extract utterances
print("Extracting utterances...")
utterances = [utterance.text for utterance in corpus.iter_utterances()]
print(f"Extracted {len(utterances)} utterances")

# Define function for emotion detection
def detect_emotions(model_name, model_path, output_path):
    """
    Process utterances with a specified emotion detection model

    Args:
        model_name: Name for reporting purposes
        model_path: HuggingFace model path
        output_path: Path to save results
    """
    print(f"\n{'-'*50}\nProcessing with {model_name} model\n{'-'*50}")

    # Load tokenizer for preprocessing
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # Clean and preprocess utterances
    def clean_text(text, tokenizer, max_length=510):
        # Handle None or empty strings
        if not text or pd.isna(text):
            return ""

        # Convert to string if needed
        if not isinstance(text, str):
            text = str(text)

        # Fix contractions
        text = contractions.fix(text)

        # Remove special characters but keep punctuation
        text = re.sub(r"[^a-zA-Z0-9\s.,!?]", "", text)

        # Tokenize and truncate if necessary
        tokens = tokenizer.tokenize(text)
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
            text = tokenizer.convert_tokens_to_string(tokens)

        return text

    print("Cleaning utterances...")
    cleaned_utterances = [clean_text(utterance, tokenizer) for utterance in tqdm(utterances)]

    # Remove empty utterances
    cleaned_utterances = [u for u in cleaned_utterances if u.strip()]
    print(f"After cleaning, kept {len(cleaned_utterances)} non-empty utterances")

    # Set up device
    device = 0 if torch.cuda.is_available() else -1
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        print("CUDA available, using GPU")
    else:
        print("CUDA not available, using CPU")

    # Load emotion classifier
    print(f"Loading {model_name} emotion classifier...")
    emotion_classifier = pipeline(
        "text-classification",
        model=model_path,
        device=device,
        batch_size=32,
        top_k=1
    )

    # Process in batches using datasets
    print("Converting to Hugging Face Dataset format...")
    dataset = Dataset.from_dict({"text": cleaned_utterances})

    # Define batch processing function
    def batch_classify(batch):
        results = emotion_classifier(batch["text"])
        return {
            "emotion": [result[0]["label"] for result in results],
            "confidence": [result[0]["score"] for result in results]
        }

    # Process the dataset
    print("Classifying emotions (this may take a while)...")
    classified_dataset = dataset.map(batch_classify, batched=True, batch_size=32)

    # Convert to pandas DataFrame
    print("Converting results to DataFrame...")
    df = pd.DataFrame({
        "utterance": cleaned_utterances,
        "emotion": classified_dataset["emotion"],
        "confidence": classified_dataset["confidence"]
    })

    # Save results
    print(f"Saving results to {output_path}...")
    df.to_csv(output_path, index=False)

    # Print summary statistics
    print("\nEmotion Distribution:")
    emotion_counts = df["emotion"].value_counts()
    for emotion, count in emotion_counts.items():
        print(f"{emotion}: {count} ({count/len(df)*100:.2f}%)")

    print("\nConfidence Statistics:")
    print(f"Mean confidence: {df['confidence'].mean():.4f}")
    print(f"Median confidence: {df['confidence'].median():.4f}")
    print(f"Min confidence: {df['confidence'].min():.4f}")
    print(f"Max confidence: {df['confidence'].max():.4f}")

    print(f"\nProcessed {len(df)} utterances. Results saved to {output_path}")

    return df

# Process with multiple models
models = [
    {
        "name": "Distilbert",
        "path": "joeddav/distilbert-base-uncased-go-emotions-student",
        "output": "/content/drive/My Drive/ML-Colab-1/Distilbert/distilbert_go_emotions_utterances.csv"
    },
    {
        "name": "Bhadresh",
        "path": "bhadresh-savani/distilbert-base-uncased-emotion",
        "output": "/content/drive/My Drive/ML-Colab-1/Bhadresh/bhadresh_emotion_utterances.csv"
    },
    {
        "name": "Roberta",
        "path": "j-hartmann/emotion-english-distilroberta-base",
        "output": "/content/drive/My Drive/ML-Colab-1/Roberta/roberta_emotion_utterances.csv"
    }
]

# Run emotion detection for each model
model_results = {}
for model in models:
    model_results[model["name"]] = detect_emotions(
        model["name"],
        model["path"],
        model["output"]
    )

"""# Normalization of three models-defined datasets"""

# Normalize datasets (common utterances)
print("\nNormalizing datasets to common utterances...")
# Find common utterances across all dataframes
common_utterances = set(model_results["Bhadresh"]["utterance"])
for model_name in model_results:
    common_utterances = common_utterances.intersection(set(model_results[model_name]["utterance"]))

# Filter each dataframe to keep only rows with common utterances
for model_name in model_results:
    model_results[model_name] = model_results[model_name][
        model_results[model_name]["utterance"].isin(common_utterances)
    ]
    # Save normalized results
    normalized_path = f"/content/drive/My Drive/ML-Colab-1/{model_name}/{model_name.lower()}_emotion_utterances_normalized.csv"
    model_results[model_name].to_csv(normalized_path, index=False)
    print(f"Saved normalized {model_name} results to {normalized_path}")

# Verify utterances are identical
print("Verifying utterance consistency...")
for i, model1 in enumerate(model_results):
    for j, model2 in enumerate(model_results):
        if i < j:
            is_equal = model_results[model1]["utterance"].equals(model_results[model2]["utterance"])
            print(f"{model1} vs {model2} utterances are equal: {is_equal}")

"""# Models Evaluation

## Creating spreadsheets for human annotation
"""

# Function to create annotation spreadsheets
def create_annotation_spreadsheet(model_name, df, reference_utterances=None, sample_size=250):
    """
    Create a spreadsheet for human annotation

    Args:
        model_name: Name of the model
        df: DataFrame with utterances and predictions
        reference_utterances: Optional list of utterances to use (for consistency)
        sample_size: Number of samples to include
    """
    print(f"\nCreating annotation spreadsheet for {model_name}...")

    # Set random seed for reproducibility
    random.seed(42)
    np.random.seed(42)

    # Handle reference utterances if provided
    if reference_utterances is not None:
        # Filter df to only include reference utterances
        df = df[df["utterance"].isin(reference_utterances)].drop_duplicates(subset=["utterance"])

        # Ensure rows are in the same order as reference_utterances
        df = df.set_index("utterance").loc[reference_utterances].reset_index()

        annotation_df = df
    else:
        # Count tokens per utterance
        df["token_count"] = df["utterance"].astype(str).apply(lambda x: len(x.split()))

        # Drop duplicates
        df = df.drop_duplicates(subset=["utterance"])

        # Get unique emotion categories
        emotion_categories = df["emotion"].unique().tolist()

        # Create a stratified sample
        stratified_sample = []
        sample_size_per_category = max(1, sample_size // len(emotion_categories))

        for emotion in emotion_categories:
            # Filter utterances with this emotion
            emotion_df = df[df["emotion"] == emotion]

            # Get high confidence samples
            high_conf_count = min(sample_size_per_category // 2, len(emotion_df))
            high_conf = emotion_df.nlargest(high_conf_count, "confidence")

            # Get low confidence samples
            low_conf_count = min(sample_size_per_category // 2, len(emotion_df))
            low_conf = emotion_df.nsmallest(low_conf_count, "confidence")

            # Add to stratified sample
            stratified_sample.extend(high_conf[["utterance", "emotion", "confidence"]].to_dict("records"))
            stratified_sample.extend(low_conf[["utterance", "emotion", "confidence"]].to_dict("records"))

        # Randomize the sample
        random.shuffle(stratified_sample)

        # Limit to desired size
        final_sample = stratified_sample[:sample_size]

        # Create DataFrame
        annotation_df = pd.DataFrame(final_sample)

        # Remove duplicate utterances
        annotation_df = annotation_df.drop_duplicates(subset=["utterance"])

        # If fewer than sample_size, add more
        if len(annotation_df) < sample_size:
            remaining_needed = sample_size - len(annotation_df)
            additional_samples = df.sample(n=remaining_needed, random_state=42)
            annotation_df = pd.concat([
                annotation_df,
                additional_samples[["utterance", "emotion", "confidence"]]
            ])

        # Ensure exactly sample_size rows
        annotation_df = annotation_df.sample(n=sample_size, random_state=42)

    # Add columns for human annotation
    annotation_df["human_annotation"] = ""
    annotation_df["confidence_level"] = ""
    annotation_df["notes"] = ""

    # Rename columns for clarity
    annotation_df = annotation_df.rename(columns={
        "emotion": "model_prediction",
        "confidence": "model_confidence"
    })

    # Reorder columns
    annotation_df = annotation_df[[
        "utterance", "human_annotation", "confidence_level",
        "model_prediction", "model_confidence", "notes"
    ]]

    # Add ID column
    annotation_df.insert(0, "id", range(1, len(annotation_df) + 1))

    # Save the annotation spreadsheet
    annotation_path = f"/content/drive/My Drive/ML-Colab-1/{model_name}/{model_name.lower()}_emotion_annotation_spreadsheet.csv"
    annotation_df.to_csv(annotation_path, index=False)
    print(f"Created annotation spreadsheet with {len(annotation_df)} samples: {annotation_path}")

    return annotation_df["utterance"].tolist()

# Create annotation spreadsheets - first for Distilbert to get reference utterances
reference_utterances = create_annotation_spreadsheet("Distilbert", model_results["Distilbert"])

# Then for other models using the same utterances
for model_name in ["Bhadresh", "Roberta"]:
    create_annotation_spreadsheet(model_name, model_results[model_name], reference_utterances)

"""## Creating Metrics for Emotion Detection Models"""

# Function to evaluate model performance
def evaluate_model(model_name, annotated_path):
    """
    Evaluate model performance based on human annotations

    Args:
        model_name: Name of the model
        annotated_path: Path to the annotated spreadsheet
    """
    print(f"\nEvaluating {model_name} model...")

    # Load annotated spreadsheet
    annotated_df = pd.read_csv(annotated_path, encoding="utf-8")

    # Make sure we have both human annotations and model predictions
    valid_df = annotated_df.dropna(subset=["human_annotation", "model_prediction"])
    print(f"Using {len(valid_df)} valid annotations out of {len(annotated_df)} total")

    # Extract ground truth and predictions
    y_true = valid_df["human_annotation"].tolist()
    y_pred = valid_df["model_prediction"].tolist()

    # Basic accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Overall Accuracy: {accuracy:.4f}")

    # Detailed classification report
    report = classification_report(y_true, y_pred, digits=3)
    print("\nClassification Report:")
    print(report)

    # Create and plot confusion matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_true, y_pred)
    emotion_labels = sorted(set(y_true + y_pred))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=emotion_labels,
                yticklabels=emotion_labels)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"{model_name} Confusion Matrix")
    plt.tight_layout()

    # Save confusion matrix
    cm_path = f"/content/drive/My Drive/ML-Colab-1/{model_name}/{model_name.lower()}_confusion_matrix.png"
    plt.savefig(cm_path)
    plt.show()

    # Calculate per-emotion accuracy
    per_emotion_accuracy = {}
    for emotion in set(y_true):
        emotion_indices = [i for i, label in enumerate(y_true) if label == emotion]
        correct = sum(1 for i in emotion_indices if y_pred[i] == y_true[i])
        per_emotion_accuracy[emotion] = correct / len(emotion_indices) if emotion_indices else 0

    print("\nAccuracy per emotion:")
    for emotion, acc in sorted(per_emotion_accuracy.items(), key=lambda x: x[1], reverse=True):
        count = y_true.count(emotion)
        print(f"{emotion}: {acc:.3f} (based on {count} samples)")

    # Analyze confidence level impact
    if "confidence_level" in valid_df.columns and valid_df["confidence_level"].notna().any():
        try:
            valid_df["confidence_level"] = pd.to_numeric(valid_df["confidence_level"])

            # Calculate accuracy by confidence level
            confidence_levels = sorted(valid_df["confidence_level"].unique())
            print("\nAccuracy by human annotation confidence:")
            for level in confidence_levels:
                level_df = valid_df[valid_df["confidence_level"] == level]
                level_acc = accuracy_score(level_df["human_annotation"], level_df["model_prediction"])
                print(f"Confidence {level}: {level_acc:.3f} (based on {len(level_df)} samples)")
        except:
            print("\nCould not analyze by confidence level (may not be numeric)")

    # Show specific examples of errors
    print("\nExamples of misclassifications:")
    errors_df = valid_df[valid_df["human_annotation"] != valid_df["model_prediction"]]
    for emotion in sorted(set(y_true)):
        error_examples = errors_df[errors_df["human_annotation"] == emotion]
        if len(error_examples) > 0:
            example = error_examples.iloc[0]
            print(f"\nTrue: {emotion}, Predicted: {example['model_prediction']}")
            print(f"Text: \"{example['utterance'][:100]}...\"")

    # If neutral is in y_true but not in model's vocabulary
    if any(x.lower() == "neutral" for x in y_true) and all(x.lower() != "neutral" for x in y_pred):
        # Calculate metrics excluding neutral examples
        non_neutral_indices = [i for i, label in enumerate(y_true) if label.lower() != "neutral"]
        non_neutral_true = [y_true[i] for i in non_neutral_indices]
        non_neutral_pred = [y_pred[i] for i in non_neutral_indices]

        print("\n\nMetrics excluding neutral utterances:")
        non_neutral_acc = accuracy_score(non_neutral_true, non_neutral_pred)
        print(f"Accuracy (excluding neutral): {non_neutral_acc:.4f}")
        print("\nClassification Report (excluding neutral):")
        non_neutral_report = classification_report(non_neutral_true, non_neutral_pred, digits=3)
        print(non_neutral_report)

        # Include non-neutral metrics in the report
        non_neutral_metrics = {
            "accuracy_excluding_neutral": non_neutral_acc,
            "report_excluding_neutral": non_neutral_report
        }
    else:
        non_neutral_metrics = None

    # Save metrics to a text file
    metrics_path = f"/content/drive/My Drive/ML-Colab-1/{model_name}/{model_name.lower()}_model_evaluation_metrics.txt"
    with open(metrics_path, "w") as f:
        f.write(f"Overall Accuracy: {accuracy:.4f}\n\n")
        f.write("Classification Report:\n")
        f.write(report)
        f.write("\n\nAccuracy per emotion:\n")
        for emotion, acc in sorted(per_emotion_accuracy.items(), key=lambda x: x[1], reverse=True):
            count = y_true.count(emotion)
            f.write(f"{emotion}: {acc:.3f} (based on {count} samples)\n")

        # Add non-neutral metrics if they exist
        if non_neutral_metrics:
            f.write("\n\nMetrics excluding neutral utterances:\n")
            f.write(f"Accuracy (excluding neutral): {non_neutral_metrics['accuracy_excluding_neutral']:.4f}\n\n")
            f.write("Classification Report (excluding neutral):\n")
            f.write(non_neutral_metrics["report_excluding_neutral"])

    print(f"\nEvaluation complete. Results saved to {metrics_path}")

annotated_files = {
    "Distilbert": "/content/drive/My Drive/ML-Colab-1/Distilbert/distilbert_emotion_annotation_spreadsheet_v1.csv",
    "Bhadresh": "/content/drive/My Drive/ML-Colab-1/Bhadresh/bhadresh_emotion_annotation_spreadsheet_v1.csv",
    "Roberta": "/content/drive/My Drive/ML-Colab-1/Roberta/roberta_emotion_annotation_spreadsheet_v1.csv"
}

# Evaluate each model
for model_name, annotated_path in annotated_files.items():
  evaluate_model(model_name, annotated_path)

print("\nAll processing completed successfully!")